---
title: "Fatal Encounters / Census Data Example"
author: "Kieran Healy"
date: "2023-02-03"
output: html_document
---

The `Rmd` file etc is here: <https://github.com/kjhealy/fatal_enc>

```{r setup, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse) # Make sure you have the latest version
library(colorspace)
library(tidycensus) # You will need a Census API key. See: https://walker-data.com/tidycensus/articles/basic-usage.html
library(tigris)
library(sf)
library(googlesheets4) # You do *not* need a token for public read-only sheets.
gs4_deauth() # Just accessing public sheets, no need for an OAUTH token

library(here)

## https://fatalencounters.org/spreadsheets/
fe_sheet <- "https://docs.google.com/spreadsheets/d/1dKmaV_JiWcG8XBoRgP8b4e9Eopkpgt7FL7nyspvzAsE/edit#gid=0"


options(tigris_use_cache = TRUE)

## Census variables from 2021 ACS5
acs5vars <- load_variables(2021, "acs5", cache = TRUE)
```


## Get the Fatal Encounters Data

We'll get the FE data live from the organization's spreadsheet. For real applications we'd get a copy and save it locally to work with. 

```{r}
## This will take a moment
fatal_df_raw <- read_sheet(ss = fe_sheet, 
                       sheet = "Form Responses") 
```

```{r}
fatal_df <- fatal_df_raw |> 
  janitor::clean_names() |> 
  rename(date = date_of_injury_resulting_in_death_month_day_year) |> 
  select(unique_id:imputation_probability, date, 
         location_of_injury_address:longitude) |> 
  relocate(unique_id, latitude, longitude, everything())
```

Let's do, say, census tracts for Los Angeles County, California. 

Filter on the California cases and convert them to a spatial data frame. This will allow us to do the spatial join in a minute. 

```{r}
fatal_ca <- fatal_df |> 
    filter(state == "CA") |> 
  st_as_sf(coords = c("longitude", "latitude"), 
           crs = "NAD83") # This is the same as the Census CRS is going to be in.

```

Now we have,

```{r}
fatal_ca 
```

See how the call to `st_as_sf()` has used the latitude/longitude columns to convert the fatal encounters table in to a proper `sf` object with a CRS and a point geometry column. This is what will let us do spatial stuff below, like merging/joining data based on whether an FE case is within a specific census tract boundary. 


## Get Census Bureau Data

Now get some tract-level data from the Census. We'll use ACS data for LA County _only_.  This next bit of code shows some handy ways to clean and reorganize the data that comes down from the Census API.

```{r}

my_vars <- tribble(
  ~variable, ~sn, 
  "B25026_001", "pop_ohu", 
  "B19013_001", "hhincome",
  "B02001_003", "total_black"
)

la_tracts <- get_acs(
  geography = "tract", 
  variables = my_vars$variable, 
  state = "CA", 
  county = "Los Angeles", 
  geometry = TRUE) |>   
  # Split the NAME field into useful columns
  # The regular expression here will need to be changed 
  # depending on the level of aggregation you are working with, because the
  # content of NAME will change correspondingly.
  separate_wider_regex(
    NAME,
    patterns = c(
      "Census Tract ", tract = "\\d+.\\d+", ", ",
      county = "[^,]+", ", ",
      state = ".*"
    )
  ) |> 
  # add the short names and widen the var, estimate, and moe cols
  left_join(my_vars, by = "variable") |> 
  # Drop original variable code
  select(-variable) |> 
  pivot_wider(names_from = sn, 
              values_from = c(estimate, moe), 
              names_glue = "{sn}_{.value}") |> 
  # tidy up
  relocate(county:geometry, .after = everything()) |> 
  st_as_sf()

```

Now we have this: 

```{r}
la_tracts
```

Notice how we have the covariates (population, income, black population, etc) spread out as columns of their own.  Also see that the `geometry` column here is polygons, not points, because tracts are areas.

## Spatial Point-in-Polygon Merge

Because we only asked the Census for LA County tract data, but we have all the fatal encounters for CA, we want to attribute the tract-level information to those rows of the Fatal Encounters data that occured in LA county. Also we want the right tract. So we use the Lat/Lon data in the Fatal Encounters table to see which tract every encounter is in and merge on that basis. This is a spatial join.

We will end up with the Census-derived data attributed to the rows of the Fatal Encounters data (where appropriate). 

```{r}
ca_pip <- st_join(fatal_ca, la_tracts, join = st_within)
```

Now we have, 

```{r}
ca_pip
```

Select a few columns just to look: 

```{r}
ca_pip |> 
  select(unique_id, date, location_of_death_county, tract, county, GEOID, hhincome_estimate, total_black_estimate)
```

You can see that `ca_pip` still pinpoints all incidents in California. But now the ones in LA have the tract variables we got from the ACS merged in. And we did this using Lat/Lon coordinates rather than trying to parse each address or whatever. Points not in LA County have `NA` for the ACS data, as we'd expect.


## Checking

But we still need to check! Let's drop all the rows missing values on `tract`, which should give us all and only FEs in LA County tracts.  

```{r}
la_pip <- ca_pip |> 
  drop_na(tract)

la_pip
```

We're down to ~1300 incidents. Draw a quick map of the points:


```{r}
ggplot(la_pip) + 
  geom_sf()
```

Looks OK. 

But! I also know there are some bad Lat/Lon records in the data, because I didn't do it this way first. Here's the `fatal_ca` table again, filtered manually by the location data in it:

```{r}
fatal_ca |> 
  filter(location_of_death_county == "Los Angeles")
```

If we draw a map of this we see something is off in the FE dataset:

```{r}
fatal_ca |> 
  filter(location_of_death_county == "Los Angeles") |> 
  ggplot() + 
  geom_sf()
```

What the hell is that point up there at >40N and >80W?

```{r}
chk <- fatal_ca |> 
  filter(location_of_death_county == "Los Angeles") |> 
  st_coordinates() |> 
  as_tibble() |> 
  mutate(bad_y = Y > 36)  

tmp <- fatal_ca |> 
  filter(location_of_death_county == "Los Angeles") |> 
  select(location_of_injury_address) 
tmp[chk$bad_y,]
```

This is the bad record. There's an Elmer Ave in North Hollywood, consistent
with the rest of the record, but the Lat and Long are wrong. (That location is in the middle
of the Barents Sea, in the Arctic.)  

This might be easy to miss in a join because the Lat/Lon doesn't correspond to an LA County tract. The row would still be in the data but if we dropped on `tract` after the join we lose that case. But that is in fact a Fatal Encounter that happened in an LA County tract, according to the FE data. It's just got the wrong Lat/Lon coordinates in that data. I'm going to ignore it for now because we're just doing a quick example. But this is the sort of thing you have to be careful about. And also, I don't like it that I found a bad geo record in the FE data on my very first random pass through it. 

## Second cut at the pointwise map:

Back to `la_pip`: 



```{r}
la_pip

# Add a "Fatal Encounter" column with the same value for every row, for the purposes of the map we're about to draw
la_pip_clean <- la_pip |> 
  mutate(fe = "Fatal Encounter")
```


```{r}
la_pip_clean |> 
  select(name, race, location_of_death_city, total_black_estimate, pop_ohu_estimate)
```


## Tracts + Points

For a model you might use e.g. an encounter-level table of data with tract-level or other features attributed to each case. Or alternatively e.g. a tract-level analysis with some summary of the number or type of encounters merged in and counted/averaged. (The unit of observation/analysis will matter a _lot_, substantively.) That's why we do the spatial join above. But for this map we'll need both tables---the  `la_tracts` table and the `la_pip_clean` table. This is because one has the tract map shapes and the other has the point locations of the FEs. The base tract map looks like this:

```{r}
la_tracts |> 
  ggplot() + 
  geom_sf()
```

We're going to put the points on top in a separate layer. The two weird tracts in the south are Santa Catalina Island and San Clemente Island. Let's get rid of those. (Again, real geography is messy ...)

```{r}
la_tracts_clean <- la_tracts |> 
  filter(!str_detect(tract, "599"))

la_tracts_clean |> 
  ggplot() + 
  geom_sf()

```


## Nicer Map

Finally, we'll fix the projections so that they're right for LA County. 

```{r}
la_tracts_clean <- st_transform(la_tracts_clean, "EPSG:6423")
la_pip_clean <- st_transform(la_pip_clean, "EPSG:6423")
```

And draw a map:

```{r}
p <- la_tracts_clean |> 
  ggplot() + 
  geom_sf(mapping = aes(fill = hhincome_estimate)) + 
  geom_sf(data = la_pip_clean, 
          mapping = aes(color = fe),  
          fill = "orange",
          pch = 21, 
          stroke = 0.2,
          size = 0.2) +
  scale_fill_binned_sequential(labels = scales::label_dollar()) +
  scale_color_manual(values = "black") + 
  guides(colour = guide_legend(override.aes = list(size=2, color = "orange"))) + 
  labs(fill = "Estimated Income", 
       color = NULL,
       title = "Point-in-Polygon Example", 
       subtitle = "ACS Income Data and Fatal Encounters Data for Los Angeles County, at the Census Tract Level") + 
  kjhslides::kjh_theme_map() 
```


```{r, fig.height=10, fig.width=12}
print(p)
```

Save it out:

```{r}
ggsave(here("figures", "pip-example.pdf"), plot = p, width = 12, height = 10)
ggsave(here("figures", "pip-example.png"), plot = p, width = 12, height = 10, bg = "white")
```

We did this example at the tract level, and just for one county. Next steps would be to do this across a variety of different geographies---`tidycensus` supports everything the Census API can deliver. 

